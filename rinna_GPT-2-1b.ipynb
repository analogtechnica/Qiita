{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rinna_GPT-2-1b.ipynb","provenance":[],"authorship_tag":"ABX9TyN1kGDtCQaLYO4CsvVr/0r1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Google Colaboratoryでりんなの大規模GPT-2(japanese-gpt-1b)を試してみる "],"metadata":{"id":"ITzSNMtV_Z-g"}},{"cell_type":"code","source":["# Transformersのインストール\n","!pip install transformers"],"metadata":{"id":"a0LziTe994qG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SentencePieceのインストール\n","!pip install sentencepiece"],"metadata":{"id":"lXDG6_NS-Fv_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ライブラリのインポート\n","import torch\n","from transformers import T5Tokenizer, AutoModelForCausalLM\n","\n","# トークナイザの読み込み\n","tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt-1b\")\n","\n","# モデル本体の読み込み\n","model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-1b\")\n","\n","# GPU使用時の設定\n","if torch.cuda.is_available():\n","    model = model.to(\"cuda\")"],"metadata":{"id":"IdU4rosV-N2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 入力文\n","text = \"GPT-2のような文章生成モデルは、\"\n","\n","# 入力文のトークナイズ\n","token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n","\n","# 入力文とパラメータの設定にしたがって文章を生成\n","with torch.no_grad():\n","    output_ids = model.generate(\n","        token_ids.to(model.device),\n","        max_length=100,\n","        min_length=100,\n","        do_sample=True,\n","        top_k=500,\n","        top_p=0.95,\n","        pad_token_id=tokenizer.pad_token_id,\n","        bos_token_id=tokenizer.bos_token_id,\n","        eos_token_id=tokenizer.eos_token_id,\n","        bad_word_ids=[[tokenizer.unk_token_id]]\n","    )\n","\n","# 生成された文章を単語IDから実際の単語に変換\n","output = tokenizer.decode(output_ids.tolist()[0])\n","print(output)"],"metadata":{"id":"ds-NbXzL_Lf6"},"execution_count":null,"outputs":[]}]}